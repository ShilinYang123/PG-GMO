#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµç¨‹å›¾æ–‡æ¡£åˆ†æå™¨
ç”¨äºåˆ†æISOç¨‹åºæ–‡ä»¶ï¼Œè¯†åˆ«é€‚åˆç”Ÿæˆæµç¨‹å›¾çš„æ–‡æ¡£

ä½œè€…: é›¨ä¿Š
åˆ›å»ºæ—¶é—´: 2025å¹´8æœˆ26æ—¥
"""

import os
import json
import logging
from pathlib import Path
from docx import Document
from datetime import datetime
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('S:\\PG-GMO\\flowchart_analyzer.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FlowchartDocumentAnalyzer:
    def __init__(self, source_dir):
        self.source_dir = Path(source_dir)
        self.analysis_results = []
        
        # æµç¨‹å›¾é€‚ç”¨æ€§å…³é”®è¯
        self.flowchart_keywords = [
            'æµç¨‹', 'ç¨‹åº', 'æ­¥éª¤', 'æ“ä½œ', 'å®¡æ‰¹', 'å®¡æ ¸', 
            'æ£€éªŒ', 'æ£€æŸ¥', 'æ§åˆ¶', 'ç®¡ç†', 'å¤„ç†', 'æ‰§è¡Œ',
            'è¯„å®¡', 'ç›‘æ§', 'æµ‹é‡', 'åˆ†æ', 'æ”¹è¿›', 'çº æ­£',
            'è¯†åˆ«', 'è¯„ä¼°', 'åº”å¯¹', 'å˜æ›´', 'å¬å›', 'æŠ•è¯‰'
        ]
        
        # ä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾çš„å…³é”®è¯
        self.exclude_keywords = [
            'è¡¨å•', 'è¡¨æ ¼', 'è®°å½•è¡¨', 'æ¸…å•', 'ç›®å½•',
            'è§„èŒƒ', 'æ ‡å‡†', 'åˆ¶åº¦', 'è§„å®š', 'è¦æ±‚'
        ]
    
    def read_docx_content(self, file_path):
        """è¯»å–DOCXæ–‡æ¡£å†…å®¹"""
        try:
            doc = Document(file_path)
            content = []
            
            # è¯»å–æ®µè½å†…å®¹
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    content.append(paragraph.text.strip())
            
            # è¯»å–è¡¨æ ¼å†…å®¹
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip():
                            content.append(cell.text.strip())
            
            return '\n'.join(content)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
            return ""
    
    def analyze_document_suitability(self, file_path, content):
        """åˆ†ææ–‡æ¡£æ˜¯å¦é€‚åˆç”Ÿæˆæµç¨‹å›¾"""
        filename = file_path.name
        
        # åŸºç¡€è¯„åˆ†
        score = 0
        reasons = []
        
        # 1. æ–‡ä»¶ååˆ†æ
        filename_lower = filename.lower()
        for keyword in self.flowchart_keywords:
            if keyword in filename_lower:
                score += 2
                reasons.append(f"æ–‡ä»¶ååŒ…å«å…³é”®è¯: {keyword}")
        
        # æ’é™¤ä¸é€‚åˆçš„æ–‡æ¡£
        for exclude_word in self.exclude_keywords:
            if exclude_word in filename_lower:
                score -= 3
                reasons.append(f"æ–‡ä»¶ååŒ…å«æ’é™¤è¯: {exclude_word}")
        
        # 2. å†…å®¹åˆ†æ
        content_lower = content.lower()
        
        # æ£€æŸ¥æµç¨‹ç›¸å…³å†…å®¹
        flowchart_indicators = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ­¥',
            r'æ­¥éª¤[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+',
            r'[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\.[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+',
            r'æµç¨‹å›¾',
            r'æ“ä½œæµç¨‹',
            r'å·¥ä½œæµç¨‹',
            r'å®¡æ‰¹æµç¨‹',
            r'å¤„ç†æµç¨‹',
            r'æ§åˆ¶æµç¨‹'
        ]
        
        for pattern in flowchart_indicators:
            matches = re.findall(pattern, content)
            if matches:
                score += len(matches) * 1
                reasons.append(f"å‘ç°æµç¨‹æŒ‡ç¤ºè¯: {pattern} ({len(matches)}æ¬¡)")
        
        # æ£€æŸ¥å…³é”®è¯é¢‘ç‡
        for keyword in self.flowchart_keywords:
            count = content_lower.count(keyword)
            if count > 0:
                score += min(count, 5)  # æœ€å¤šåŠ 5åˆ†
                if count >= 3:
                    reasons.append(f"å…³é”®è¯'{keyword}'å‡ºç°{count}æ¬¡")
        
        # 3. æ–‡æ¡£ç»“æ„åˆ†æ
        lines = content.split('\n')
        numbered_lines = 0
        for line in lines:
            if re.match(r'^[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.\.ã€]', line.strip()):
                numbered_lines += 1
        
        if numbered_lines >= 3:
            score += 3
            reasons.append(f"å‘ç°{numbered_lines}ä¸ªç¼–å·æ¡ç›®")
        
        # 4. ç‰¹æ®Šç¨‹åºæ–‡ä»¶åŠ åˆ†
        special_procedures = [
            'æ§åˆ¶ç¨‹åº', 'ç®¡ç†ç¨‹åº', 'è¯„å®¡ç¨‹åº', 'å®¡æ ¸ç¨‹åº',
            'æ£€éªŒç¨‹åº', 'å¤„ç†ç¨‹åº', 'åº”å¯¹ç¨‹åº'
        ]
        
        for procedure in special_procedures:
            if procedure in filename:
                score += 3
                reasons.append(f"ç‰¹æ®Šç¨‹åºæ–‡ä»¶: {procedure}")
        
        # åˆ¤æ–­æ˜¯å¦é€‚åˆç”Ÿæˆæµç¨‹å›¾
        suitable = score >= 5
        
        return {
            'suitable': suitable,
            'score': score,
            'reasons': reasons,
            'content_length': len(content),
            'line_count': len(lines)
        }
    
    def scan_documents(self):
        """æ‰«ææ‰€æœ‰DOCXæ–‡æ¡£"""
        logger.info(f"å¼€å§‹æ‰«æç›®å½•: {self.source_dir}")
        
        docx_files = list(self.source_dir.rglob('*.docx'))
        logger.info(f"æ‰¾åˆ° {len(docx_files)} ä¸ªDOCXæ–‡ä»¶")
        
        suitable_count = 0
        unsuitable_count = 0
        
        for file_path in docx_files:
            # è·³è¿‡ä¸´æ—¶æ–‡ä»¶
            if file_path.name.startswith('~$'):
                continue
            
            logger.info(f"åˆ†ææ–‡æ¡£: {file_path.name}")
            
            # è¯»å–æ–‡æ¡£å†…å®¹
            content = self.read_docx_content(file_path)
            
            if not content:
                logger.warning(f"æ–‡æ¡£å†…å®¹ä¸ºç©º: {file_path.name}")
                continue
            
            # åˆ†æé€‚ç”¨æ€§
            analysis = self.analyze_document_suitability(file_path, content)
            
            # è®°å½•åˆ†æç»“æœ
            result = {
                'file_path': str(file_path),
                'file_name': file_path.name,
                'relative_path': str(file_path.relative_to(self.source_dir)),
                'suitable_for_flowchart': analysis['suitable'],
                'suitability_score': analysis['score'],
                'reasons': analysis['reasons'],
                'content_length': analysis['content_length'],
                'line_count': analysis['line_count'],
                'analysis_time': datetime.now().isoformat()
            }
            
            self.analysis_results.append(result)
            
            if analysis['suitable']:
                suitable_count += 1
                logger.info(f"âœ… é€‚åˆç”Ÿæˆæµç¨‹å›¾: {file_path.name} (è¯„åˆ†: {analysis['score']})")
            else:
                unsuitable_count += 1
                logger.info(f"âŒ ä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾: {file_path.name} (è¯„åˆ†: {analysis['score']})")
        
        logger.info(f"æ‰«æå®Œæˆ: é€‚åˆ {suitable_count} ä¸ª, ä¸é€‚åˆ {unsuitable_count} ä¸ª")
        return self.analysis_results
    
    def generate_analysis_report(self, output_path):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        suitable_docs = [doc for doc in self.analysis_results if doc['suitable_for_flowchart']]
        unsuitable_docs = [doc for doc in self.analysis_results if not doc['suitable_for_flowchart']]
        
        report = {
            'analysis_summary': {
                'total_documents': len(self.analysis_results),
                'suitable_documents': len(suitable_docs),
                'unsuitable_documents': len(unsuitable_docs),
                'analysis_time': datetime.now().isoformat()
            },
            'suitable_documents': suitable_docs,
            'unsuitable_documents': unsuitable_docs
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_path = output_path.with_suffix('.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(report, output_path)
        
        logger.info(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        return report
    
    def generate_markdown_report(self, report, output_path):
        """ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š"""
        md_content = f"""# æµç¨‹å›¾æ–‡æ¡£åˆ†ææŠ¥å‘Š

## ğŸ“Š åˆ†ææ¦‚è¦

- **åˆ†ææ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- **æ–‡æ¡£æ€»æ•°**: {report['analysis_summary']['total_documents']} ä¸ª
- **é€‚åˆç”Ÿæˆæµç¨‹å›¾**: {report['analysis_summary']['suitable_documents']} ä¸ª
- **ä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾**: {report['analysis_summary']['unsuitable_documents']} ä¸ª
- **é€‚ç”¨ç‡**: {report['analysis_summary']['suitable_documents'] / report['analysis_summary']['total_documents'] * 100:.1f}%

## âœ… é€‚åˆç”Ÿæˆæµç¨‹å›¾çš„æ–‡æ¡£

| åºå· | æ–‡æ¡£åç§° | è¯„åˆ† | ä¸»è¦åŸå›  |
|------|----------|------|----------|
"""
        
        for i, doc in enumerate(report['suitable_documents'], 1):
            reasons = '; '.join(doc['reasons'][:3])  # åªæ˜¾ç¤ºå‰3ä¸ªåŸå› 
            md_content += f"| {i} | {doc['file_name']} | {doc['suitability_score']} | {reasons} |\n"
        
        md_content += f"""

## âŒ ä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾çš„æ–‡æ¡£

| åºå· | æ–‡æ¡£åç§° | è¯„åˆ† | ä¸»è¦åŸå›  |
|------|----------|------|----------|
"""
        
        for i, doc in enumerate(report['unsuitable_documents'], 1):
            reasons = '; '.join(doc['reasons'][:3]) if doc['reasons'] else 'è¯„åˆ†è¿‡ä½'
            md_content += f"| {i} | {doc['file_name']} | {doc['suitability_score']} | {reasons} |\n"
        
        md_content += f"""

## ğŸ“‹ è¯¦ç»†åˆ†æç»“æœ

### é€‚åˆç”Ÿæˆæµç¨‹å›¾çš„æ–‡æ¡£è¯¦æƒ…

"""
        
        for doc in report['suitable_documents']:
            md_content += f"""
#### {doc['file_name']}
- **è·¯å¾„**: {doc['relative_path']}
- **é€‚ç”¨æ€§è¯„åˆ†**: {doc['suitability_score']}
- **å†…å®¹é•¿åº¦**: {doc['content_length']} å­—ç¬¦
- **è¡Œæ•°**: {doc['line_count']} è¡Œ
- **åˆ†æåŸå› **:
"""
            for reason in doc['reasons']:
                md_content += f"  - {reason}\n"
            md_content += "\n"
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)

def main():
    """ä¸»å‡½æ•°"""
    source_dir = "S:\\PG-GMO\\01-Input\\åŸå§‹æ–‡æ¡£\\PG-ISOæ–‡ä»¶_docx"
    output_dir = Path("S:\\PG-GMO\\02-Output")
    output_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = FlowchartDocumentAnalyzer(source_dir)
    
    # æ‰«ææ–‡æ¡£
    results = analyzer.scan_documents()
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = output_dir / "æµç¨‹å›¾æ–‡æ¡£åˆ†ææŠ¥å‘Š.md"
    report = analyzer.generate_analysis_report(report_path)
    
    print(f"\nğŸ“Š åˆ†æå®Œæˆ!")
    print(f"æ€»æ–‡æ¡£æ•°: {report['analysis_summary']['total_documents']}")
    print(f"é€‚åˆç”Ÿæˆæµç¨‹å›¾: {report['analysis_summary']['suitable_documents']}")
    print(f"ä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾: {report['analysis_summary']['unsuitable_documents']}")
    print(f"æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    
    return report

if __name__ == "__main__":
    main()