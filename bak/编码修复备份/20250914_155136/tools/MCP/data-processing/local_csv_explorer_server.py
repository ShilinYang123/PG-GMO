#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ°CSVæ•°æ®æ¢ç´¢MCPæœåŠ¡å™¨

åŠŸèƒ½ï¼š
1. CSVæ–‡ä»¶è¯»å–å’Œè§£æ
2. æ•°æ®ç»Ÿè®¡åˆ†æ
3. æ•°æ®è´¨é‡æ£€æŸ¥
4. æ•°æ®æ¸…æ´—å»ºè®®
5. æ•°æ®å¯è§†åŒ–ä¿¡æ¯
6. å¤§æ–‡ä»¶åˆ†å—å¤„ç†
"""

import json
import logging
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import chardet
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class LocalCSVExplorer:
    def __init__(self, max_rows: int = 10000):
        self.max_rows = max_rows
        self.logger = logging.getLogger(__name__)
        self.loaded_files = {}
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    
    def detect_encoding(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)  # è¯»å–å‰10KBæ£€æµ‹ç¼–ç 
                result = chardet.detect(raw_data)
                return result.get('encoding', 'utf-8')
        except Exception as e:
            self.logger.warning(f"ç¼–ç æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤UTF-8: {e}")
            return 'utf-8'
    
    def load_csv(self, file_path: str, **kwargs) -> Dict:
        """åŠ è½½CSVæ–‡ä»¶"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                return {"status": "error", "message": "æ–‡ä»¶ä¸å­˜åœ¨"}
            
            # æ£€æµ‹ç¼–ç 
            encoding = kwargs.get('encoding') or self.detect_encoding(str(file_path))
            
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(
                file_path,
                encoding=encoding,
                nrows=self.max_rows,
                **{k: v for k, v in kwargs.items() if k != 'encoding'}
            )
            
            # å­˜å‚¨åˆ°å†…å­˜
            file_key = str(file_path)
            self.loaded_files[file_key] = {
                'dataframe': df,
                'file_path': file_key,
                'loaded_at': datetime.now().isoformat(),
                'encoding': encoding,
                'original_shape': df.shape
            }
            
            return {
                "status": "success",
                "message": f"æˆåŠŸåŠ è½½CSVæ–‡ä»¶: {file_path.name}",
                "file_key": file_key,
                "shape": df.shape,
                "columns": df.columns.tolist(),
                "encoding": encoding
            }
        except Exception as e:
            return {"status": "error", "message": f"åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {str(e)}"}
    
    def get_basic_info(self, file_key: str) -> Dict:
        """è·å–æ•°æ®åŸºæœ¬ä¿¡æ¯"""
        try:
            if file_key not in self.loaded_files:
                return {"status": "error", "message": "æ–‡ä»¶æœªåŠ è½½"}
            
            df = self.loaded_files[file_key]['dataframe']
            
            # åŸºæœ¬ä¿¡æ¯
            info = {
                "shape": df.shape,
                "columns": df.columns.tolist(),
                "dtypes": df.dtypes.astype(str).to_dict(),
                "memory_usage": df.memory_usage(deep=True).sum(),
                "null_counts": df.isnull().sum().to_dict(),
                "duplicate_rows": df.duplicated().sum()
            }
            
            # æ•°æ®ç±»å‹ç»Ÿè®¡
            dtype_counts = df.dtypes.value_counts().to_dict()
            info["dtype_summary"] = {str(k): v for k, v in dtype_counts.items()}
            
            return {
                "status": "success",
                "info": info,
                "file_info": {
                    "file_path": self.loaded_files[file_key]['file_path'],
                    "loaded_at": self.loaded_files[file_key]['loaded_at'],
                    "encoding": self.loaded_files[file_key]['encoding']
                }
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def get_statistical_summary(self, file_key: str, columns: Optional[List[str]] = None) -> Dict:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        try:
            if file_key not in self.loaded_files:
                return {"status": "error", "message": "æ–‡ä»¶æœªåŠ è½½"}
            
            df = self.loaded_files[file_key]['dataframe']
            
            if columns:
                df = df[columns]
            
            # æ•°å€¼åˆ—ç»Ÿè®¡
            numeric_summary = {}
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                numeric_summary = df[numeric_cols].describe().to_dict()
            
            # åˆ†ç±»åˆ—ç»Ÿè®¡
            categorical_summary = {}
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns
            for col in categorical_cols:
                categorical_summary[col] = {
                    "unique_count": df[col].nunique(),
                    "most_frequent": df[col].mode().iloc[0] if not df[col].mode().empty else None,
                    "value_counts": df[col].value_counts().head(10).to_dict()
                }
            
            return {
                "status": "success",
                "numeric_summary": numeric_summary,
                "categorical_summary": categorical_summary,
                "column_types": {
                    "numeric": numeric_cols.tolist(),
                    "categorical": categorical_cols.tolist()
                }
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def check_data_quality(self, file_key: str) -> Dict:
        """æ•°æ®è´¨é‡æ£€æŸ¥"""
        try:
            if file_key not in self.loaded_files:
                return {"status": "error", "message": "æ–‡ä»¶æœªåŠ è½½"}
            
            df = self.loaded_files[file_key]['dataframe']
            
            quality_issues = []
            recommendations = []
            
            # æ£€æŸ¥ç¼ºå¤±å€¼
            missing_data = df.isnull().sum()
            high_missing_cols = missing_data[missing_data > len(df) * 0.5].index.tolist()
            if high_missing_cols:
                quality_issues.append(f"é«˜ç¼ºå¤±ç‡åˆ—: {high_missing_cols}")
                recommendations.append("è€ƒè™‘åˆ é™¤æˆ–å¡«å……é«˜ç¼ºå¤±ç‡åˆ—")
            
            # æ£€æŸ¥é‡å¤è¡Œ
            duplicate_count = df.duplicated().sum()
            if duplicate_count > 0:
                quality_issues.append(f"é‡å¤è¡Œæ•°: {duplicate_count}")
                recommendations.append("è€ƒè™‘åˆ é™¤é‡å¤è¡Œ")
            
            # æ£€æŸ¥æ•°æ®ç±»å‹
            for col in df.columns:
                if df[col].dtype == 'object':
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ˜¯æ•°å€¼ç±»å‹
                    try:
                        pd.to_numeric(df[col], errors='raise')
                        quality_issues.append(f"åˆ— '{col}' å¯èƒ½åº”è¯¥æ˜¯æ•°å€¼ç±»å‹")
                        recommendations.append(f"è€ƒè™‘å°†åˆ— '{col}' è½¬æ¢ä¸ºæ•°å€¼ç±»å‹")
                    except:
                        pass
            
            # æ£€æŸ¥å¼‚å¸¸å€¼ï¼ˆä»…æ•°å€¼åˆ—ï¼‰
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            outlier_info = {}
            for col in numeric_cols:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                if len(outliers) > 0:
                    outlier_info[col] = len(outliers)
            
            if outlier_info:
                quality_issues.append(f"å¼‚å¸¸å€¼æ£€æµ‹: {outlier_info}")
                recommendations.append("æ£€æŸ¥å¹¶å¤„ç†å¼‚å¸¸å€¼")
            
            return {
                "status": "success",
                "quality_score": max(0, 100 - len(quality_issues) * 10),
                "issues": quality_issues,
                "recommendations": recommendations,
                "detailed_checks": {
                    "missing_data": missing_data.to_dict(),
                    "duplicate_rows": int(duplicate_count),
                    "outliers": outlier_info
                }
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def get_sample_data(self, file_key: str, n_rows: int = 10) -> Dict:
        """è·å–æ ·æœ¬æ•°æ®"""
        try:
            if file_key not in self.loaded_files:
                return {"status": "error", "message": "æ–‡ä»¶æœªåŠ è½½"}
            
            df = self.loaded_files[file_key]['dataframe']
            
            sample_data = {
                "head": df.head(n_rows).to_dict('records'),
                "tail": df.tail(n_rows).to_dict('records'),
                "random_sample": df.sample(min(n_rows, len(df))).to_dict('records')
            }
            
            return {
                "status": "success",
                "sample_data": sample_data,
                "total_rows": len(df)
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def filter_data(self, file_key: str, conditions: Dict) -> Dict:
        """æ•°æ®è¿‡æ»¤"""
        try:
            if file_key not in self.loaded_files:
                return {"status": "error", "message": "æ–‡ä»¶æœªåŠ è½½"}
            
            df = self.loaded_files[file_key]['dataframe'].copy()
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            for column, condition in conditions.items():
                if column not in df.columns:
                    continue
                
                if isinstance(condition, dict):
                    if 'min' in condition:
                        df = df[df[column] >= condition['min']]
                    if 'max' in condition:
                        df = df[df[column] <= condition['max']]
                    if 'equals' in condition:
                        df = df[df[column] == condition['equals']]
                    if 'contains' in condition:
                        df = df[df[column].astype(str).str.contains(condition['contains'], na=False)]
            
            return {
                "status": "success",
                "filtered_shape": df.shape,
                "original_shape": self.loaded_files[file_key]['dataframe'].shape,
                "sample_data": df.head(10).to_dict('records')
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def get_correlation_matrix(self, file_key: str) -> Dict:
        """è·å–ç›¸å…³æ€§çŸ©é˜µ"""
        try:
            if file_key not in self.loaded_files:
                return {"status": "error", "message": "æ–‡ä»¶æœªåŠ è½½"}
            
            df = self.loaded_files[file_key]['dataframe']
            numeric_df = df.select_dtypes(include=[np.number])
            
            if numeric_df.empty:
                return {"status": "error", "message": "æ²¡æœ‰æ•°å€¼åˆ—å¯è®¡ç®—ç›¸å…³æ€§"}
            
            correlation_matrix = numeric_df.corr().to_dict()
            
            # æ‰¾å‡ºé«˜ç›¸å…³æ€§å¯¹
            high_correlations = []
            for col1 in correlation_matrix:
                for col2 in correlation_matrix[col1]:
                    if col1 != col2 and abs(correlation_matrix[col1][col2]) > 0.7:
                        high_correlations.append({
                            "column1": col1,
                            "column2": col2,
                            "correlation": correlation_matrix[col1][col2]
                        })
            
            return {
                "status": "success",
                "correlation_matrix": correlation_matrix,
                "high_correlations": high_correlations,
                "numeric_columns": numeric_df.columns.tolist()
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def list_loaded_files(self) -> Dict:
        """åˆ—å‡ºå·²åŠ è½½çš„æ–‡ä»¶"""
        try:
            files_info = []
            for file_key, info in self.loaded_files.items():
                files_info.append({
                    "file_key": file_key,
                    "file_path": info['file_path'],
                    "shape": info['original_shape'],
                    "loaded_at": info['loaded_at'],
                    "encoding": info['encoding']
                })
            
            return {
                "status": "success",
                "loaded_files": files_info,
                "total_files": len(files_info)
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}

class CSVExplorerMCPServer:
    def __init__(self, max_rows: int = 10000):
        self.explorer = LocalCSVExplorer(max_rows)
        self.logger = logging.getLogger(__name__)
    
    def get_available_functions(self) -> Dict:
        """è·å–å¯ç”¨åŠŸèƒ½åˆ—è¡¨"""
        return {
            "functions": [
                {
                    "name": "load_csv",
                    "description": "åŠ è½½CSVæ–‡ä»¶åˆ°å†…å­˜",
                    "parameters": {
                        "file_path": "CSVæ–‡ä»¶è·¯å¾„",
                        "encoding": "æ–‡ä»¶ç¼–ç ï¼ˆå¯é€‰ï¼‰",
                        "separator": "åˆ†éš”ç¬¦ï¼ˆå¯é€‰ï¼‰"
                    }
                },
                {
                    "name": "get_basic_info",
                    "description": "è·å–æ•°æ®åŸºæœ¬ä¿¡æ¯",
                    "parameters": {
                        "file_key": "æ–‡ä»¶æ ‡è¯†ç¬¦"
                    }
                },
                {
                    "name": "get_statistical_summary",
                    "description": "è·å–ç»Ÿè®¡æ‘˜è¦",
                    "parameters": {
                        "file_key": "æ–‡ä»¶æ ‡è¯†ç¬¦",
                        "columns": "æŒ‡å®šåˆ—ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰"
                    }
                },
                {
                    "name": "check_data_quality",
                    "description": "æ•°æ®è´¨é‡æ£€æŸ¥",
                    "parameters": {
                        "file_key": "æ–‡ä»¶æ ‡è¯†ç¬¦"
                    }
                },
                {
                    "name": "get_sample_data",
                    "description": "è·å–æ ·æœ¬æ•°æ®",
                    "parameters": {
                        "file_key": "æ–‡ä»¶æ ‡è¯†ç¬¦",
                        "n_rows": "æ ·æœ¬è¡Œæ•°ï¼ˆå¯é€‰ï¼‰"
                    }
                },
                {
                    "name": "filter_data",
                    "description": "æ•°æ®è¿‡æ»¤",
                    "parameters": {
                        "file_key": "æ–‡ä»¶æ ‡è¯†ç¬¦",
                        "conditions": "è¿‡æ»¤æ¡ä»¶å­—å…¸"
                    }
                },
                {
                    "name": "get_correlation_matrix",
                    "description": "è·å–ç›¸å…³æ€§çŸ©é˜µ",
                    "parameters": {
                        "file_key": "æ–‡ä»¶æ ‡è¯†ç¬¦"
                    }
                },
                {
                    "name": "list_loaded_files",
                    "description": "åˆ—å‡ºå·²åŠ è½½çš„æ–‡ä»¶",
                    "parameters": {}
                }
            ]
        }
    
    def execute_function(self, function_name: str, **kwargs) -> Dict:
        """æ‰§è¡ŒæŒ‡å®šåŠŸèƒ½"""
        try:
            if function_name == "load_csv":
                return self.explorer.load_csv(
                    kwargs.get("file_path"),
                    **{k: v for k, v in kwargs.items() if k != "file_path"}
                )
            elif function_name == "get_basic_info":
                return self.explorer.get_basic_info(kwargs.get("file_key"))
            elif function_name == "get_statistical_summary":
                return self.explorer.get_statistical_summary(
                    kwargs.get("file_key"),
                    kwargs.get("columns")
                )
            elif function_name == "check_data_quality":
                return self.explorer.check_data_quality(kwargs.get("file_key"))
            elif function_name == "get_sample_data":
                return self.explorer.get_sample_data(
                    kwargs.get("file_key"),
                    kwargs.get("n_rows", 10)
                )
            elif function_name == "filter_data":
                return self.explorer.filter_data(
                    kwargs.get("file_key"),
                    kwargs.get("conditions", {})
                )
            elif function_name == "get_correlation_matrix":
                return self.explorer.get_correlation_matrix(kwargs.get("file_key"))
            elif function_name == "list_loaded_files":
                return self.explorer.list_loaded_files()
            else:
                return {"status": "error", "message": f"æœªçŸ¥åŠŸèƒ½: {function_name}"}
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def create_demo_data(self) -> Dict:
        """åˆ›å»ºæ¼”ç¤ºæ•°æ®"""
        try:
            # åˆ›å»ºæ¼”ç¤ºCSVæ–‡ä»¶
            demo_data = {
                'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ', 'å­™å…«', 'å‘¨ä¹', 'å´å'],
                'å¹´é¾„': [25, 30, 35, 28, 32, 27, 29, 31],
                'åŸå¸‚': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'å—äº¬', 'æˆéƒ½', 'æ­¦æ±‰'],
                'è–ªèµ„': [8000, 12000, 15000, 11000, 13000, 9000, 10000, 14000],
                'éƒ¨é—¨': ['æŠ€æœ¯', 'é”€å”®', 'å¸‚åœº', 'æŠ€æœ¯', 'é”€å”®', 'æŠ€æœ¯', 'å¸‚åœº', 'æŠ€æœ¯'],
                'å…¥èŒæ—¥æœŸ': ['2020-01-15', '2019-03-20', '2018-06-10', '2021-02-28', 
                           '2020-08-05', '2021-05-12', '2019-11-30', '2020-12-18']
            }
            
            df = pd.DataFrame(demo_data)
            demo_file_path = Path("./demo_employees.csv")
            df.to_csv(demo_file_path, index=False, encoding='utf-8')
            
            # åŠ è½½æ¼”ç¤ºæ–‡ä»¶
            load_result = self.explorer.load_csv(str(demo_file_path))
            
            return {
                "status": "success",
                "message": "æ¼”ç¤ºæ•°æ®åˆ›å»ºæˆåŠŸ",
                "demo_file": str(demo_file_path),
                "load_result": load_result
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæœåŠ¡å™¨åŠŸèƒ½"""
    print("ğŸš€ å¯åŠ¨æœ¬åœ°CSVæ•°æ®æ¢ç´¢MCPæœåŠ¡å™¨...")
    
    server = CSVExplorerMCPServer(max_rows=10000)
    
    # æ˜¾ç¤ºå¯ç”¨åŠŸèƒ½
    functions = server.get_available_functions()
    print("\nğŸ“‹ å¯ç”¨åŠŸèƒ½:")
    for func in functions["functions"]:
        print(f"  - {func['name']}: {func['description']}")
    
    # åˆ›å»ºæ¼”ç¤ºæ•°æ®
    print("\nğŸ¯ åˆ›å»ºæ¼”ç¤ºæ•°æ®...")
    demo_result = server.create_demo_data()
    print(f"æ¼”ç¤ºæ•°æ®åˆ›å»ºç»“æœ: {demo_result['message']}")
    
    if demo_result["status"] == "success":
        file_key = demo_result["load_result"]["file_key"]
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        print("\nğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯:")
        basic_info = server.execute_function("get_basic_info", file_key=file_key)
        if basic_info["status"] == "success":
            info = basic_info["info"]
            print(f"  - æ•°æ®å½¢çŠ¶: {info['shape']}")
            print(f"  - åˆ—å: {info['columns']}")
            print(f"  - ç¼ºå¤±å€¼: {sum(info['null_counts'].values())}")
        
        # ç»Ÿè®¡æ‘˜è¦
        print("\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
        stats = server.execute_function("get_statistical_summary", file_key=file_key)
        if stats["status"] == "success":
            print(f"  - æ•°å€¼åˆ—: {stats['column_types']['numeric']}")
            print(f"  - åˆ†ç±»åˆ—: {stats['column_types']['categorical']}")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        print("\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥:")
        quality = server.execute_function("check_data_quality", file_key=file_key)
        if quality["status"] == "success":
            print(f"  - è´¨é‡è¯„åˆ†: {quality['quality_score']}/100")
            if quality['issues']:
                print(f"  - å‘ç°é—®é¢˜: {quality['issues']}")
            if quality['recommendations']:
                print(f"  - å»ºè®®: {quality['recommendations']}")
        
        # æ ·æœ¬æ•°æ®
        print("\nğŸ“‹ æ ·æœ¬æ•°æ®:")
        sample = server.execute_function("get_sample_data", file_key=file_key, n_rows=3)
        if sample["status"] == "success":
            print("  å‰3è¡Œæ•°æ®:")
            for i, row in enumerate(sample["sample_data"]["head"]):
                print(f"    {i+1}: {row}")
    
    print("\nâœ… æœ¬åœ°CSVæ•°æ®æ¢ç´¢MCPæœåŠ¡å™¨æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. å¯¼å…¥CSVExplorerMCPServerç±»")
    print("2. åˆ›å»ºæœåŠ¡å™¨å®ä¾‹: server = CSVExplorerMCPServer()")
    print("3. åŠ è½½CSVæ–‡ä»¶: server.execute_function('load_csv', file_path='path/to/file.csv')")
    print("4. ä½¿ç”¨å„ç§åˆ†æåŠŸèƒ½æ¢ç´¢æ•°æ®")

if __name__ == "__main__":
    main()