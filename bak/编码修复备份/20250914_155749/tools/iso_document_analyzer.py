#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ISOæ–‡æ¡£åˆ†æå·¥å…·
ç”¨äºåˆ†æå“é«˜ISOç¨‹åºæ–‡æ¡£ï¼Œåˆ¤æ–­æµç¨‹å›¾ç”Ÿæˆé€‚ç”¨æ€§
åŸºäºæ–‡ä»¶åå’ŒISOæ ‡å‡†ç¨‹åºç‰¹å¾è¿›è¡Œæ™ºèƒ½åˆ¤æ–­
"""

import os
import sys
import json
from pathlib import Path
from collections import defaultdict
import re

class ISODocumentAnalyzer:
    def __init__(self):
        self.base_path = Path(r'S:\PG-GMO\01-Input\åŸå§‹æ–‡æ¡£\PG-ISOæ–‡ä»¶')
        self.output_path = Path(r'S:\PG-GMO\02-Output')
        
        # åŸºäºISOæ ‡å‡†å’Œæ–‡ä»¶åçš„é€‚ç”¨æ€§è§„åˆ™
        self.flowchart_suitable_patterns = {
            # é«˜é€‚ç”¨æ€§ - æ˜ç¡®çš„æµç¨‹æ§åˆ¶ç¨‹åº
            'high': [
                ('æ§åˆ¶ç¨‹åº', 'ç®¡ç†æµç¨‹ç±»'),
                ('ç®¡ç†ç¨‹åº', 'ç®¡ç†æµç¨‹ç±»'),
                ('è¯„å®¡', 'å®¡æ‰¹æµç¨‹ç±»'),
                ('å®¡æ ¸', 'å®¡æ‰¹æµç¨‹ç±»'),
                ('æ£€éªŒ', 'æ£€éªŒç¨‹åºç±»'),
                ('æ£€æŸ¥', 'æ£€éªŒç¨‹åºç±»'),
                ('ç›‘æ§', 'è´¨é‡æ§åˆ¶ç±»'),
                ('æµ‹é‡', 'æ£€éªŒç¨‹åºç±»'),
                ('çº æ­£', 'è´¨é‡æ§åˆ¶ç±»'),
                ('æŠ•è¯‰å¤„ç†', 'å®¢æˆ·æœåŠ¡ç±»'),
                ('å¬å›', 'åº”æ€¥å¤„ç†ç±»'),
                ('é£é™©è¯„ä¼°', 'é£é™©ç®¡ç†ç±»'),
                ('åº”å¯¹', 'é£é™©ç®¡ç†ç±»'),
                ('å˜æ›´', 'å˜æ›´ç®¡ç†ç±»'),
                ('å¼‚å¸¸å¤„ç†', 'å¼‚å¸¸å¤„ç†ç±»'),
                ('åº”å˜æªæ–½', 'åº”æ€¥å¤„ç†ç±»')
            ],
            # ä¸­ç­‰é€‚ç”¨æ€§ - å¯èƒ½åŒ…å«æµç¨‹çš„ç¨‹åº
            'medium': [
                ('èµ„æº', 'èµ„æºç®¡ç†ç±»'),
                ('è®¾å¤‡', 'è®¾å¤‡ç®¡ç†ç±»'),
                ('è®¾æ–½', 'è®¾å¤‡ç®¡ç†ç±»'),
                ('è®¢å•', 'ä¸šåŠ¡æµç¨‹ç±»'),
                ('è®¾è®¡å¼€å‘', 'äº§å“å¼€å‘ç±»'),
                ('å¤–éƒ¨æä¾›', 'ä¾›åº”å•†ç®¡ç†ç±»'),
                ('ç”Ÿäº§', 'ç”Ÿäº§ç®¡ç†ç±»'),
                ('æ ‡è¯†', 'æ ‡è¯†ç®¡ç†ç±»'),
                ('å¯è¿½æº¯', 'è¿½æº¯ç®¡ç†ç±»'),
                ('æ”¾è¡Œ', 'è´¨é‡æ§åˆ¶ç±»'),
                ('æ»¡æ„', 'å®¢æˆ·æœåŠ¡ç±»'),
                ('è®¤è¯', 'è®¤è¯ç®¡ç†ç±»'),
                ('é‡‡è´­', 'é‡‡è´­ç®¡ç†ç±»'),
                ('ä¾›åº”å•†', 'ä¾›åº”å•†ç®¡ç†ç±»')
            ],
            # ä½é€‚ç”¨æ€§ - ä¸»è¦æ˜¯ç®¡ç†åˆ¶åº¦
            'low': [
                ('äººåŠ›èµ„æº', 'äººäº‹ç®¡ç†ç±»'),
                ('çŸ¥è¯†ç®¡ç†', 'çŸ¥è¯†ç®¡ç†ç±»'),
                ('æ ‡å¿—ç®¡ç†', 'æ ‡è¯†ç®¡ç†ç±»'),
                ('å°¾å•å¤„ç†', 'ä¸šåŠ¡ç®¡ç†ç±»')
            ]
        }
        
        self.analysis_results = []
    
    def analyze_document_by_filename(self, filename, file_path):
        """åŸºäºæ–‡ä»¶ååˆ†ææ–‡æ¡£é€‚ç”¨æ€§"""
        filename_lower = filename.lower()
        
        # æ£€æŸ¥é«˜é€‚ç”¨æ€§æ¨¡å¼
        for pattern, category in self.flowchart_suitable_patterns['high']:
            if pattern in filename_lower:
                return {
                    'suitable': True,
                    'reason': f'åŒ…å«é«˜é€‚ç”¨æ€§å…³é”®è¯"{pattern}"ï¼Œå±äº{category}ï¼Œé€‚åˆç”Ÿæˆæµç¨‹å›¾',
                    'category': category,
                    'confidence': 90,
                    'pattern_matched': pattern,
                    'suitability_level': 'high'
                }
        
        # æ£€æŸ¥ä¸­ç­‰é€‚ç”¨æ€§æ¨¡å¼
        for pattern, category in self.flowchart_suitable_patterns['medium']:
            if pattern in filename_lower:
                return {
                    'suitable': True,
                    'reason': f'åŒ…å«ä¸­ç­‰é€‚ç”¨æ€§å…³é”®è¯"{pattern}"ï¼Œå±äº{category}ï¼Œé€‚åˆç”Ÿæˆæµç¨‹å›¾',
                    'category': category,
                    'confidence': 70,
                    'pattern_matched': pattern,
                    'suitability_level': 'medium'
                }
        
        # æ£€æŸ¥ä½é€‚ç”¨æ€§æ¨¡å¼
        for pattern, category in self.flowchart_suitable_patterns['low']:
            if pattern in filename_lower:
                return {
                    'suitable': False,
                    'reason': f'åŒ…å«ä½é€‚ç”¨æ€§å…³é”®è¯"{pattern}"ï¼Œå±äº{category}ï¼Œä¸»è¦ä¸ºç®¡ç†åˆ¶åº¦ï¼Œä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾',
                    'category': category,
                    'confidence': 30,
                    'pattern_matched': pattern,
                    'suitability_level': 'low'
                }
        
        # é»˜è®¤åˆ¤æ–­ - æ‰€æœ‰HQ-QPç¨‹åºéƒ½æœ‰ä¸€å®šæµç¨‹æ€§
        if filename.startswith('HQ-QP-'):
            return {
                'suitable': True,
                'reason': 'ISOè´¨é‡ç¨‹åºæ–‡æ¡£ï¼Œé»˜è®¤åŒ…å«ç®¡ç†æµç¨‹ï¼Œé€‚åˆç”Ÿæˆæµç¨‹å›¾',
                'category': 'æ ‡å‡†ç¨‹åºç±»',
                'confidence': 60,
                'pattern_matched': 'HQ-QP-',
                'suitability_level': 'default'
            }
        
        return {
            'suitable': False,
            'reason': 'æœªåŒ¹é…åˆ°å·²çŸ¥çš„æµç¨‹æ¨¡å¼',
            'category': 'æœªçŸ¥ç±»å‹',
            'confidence': 0,
            'pattern_matched': None,
            'suitability_level': 'unknown'
        }
    
    def analyze_all_documents(self):
        """åˆ†ææ‰€æœ‰ä¸»è¦ç¨‹åºæ–‡æ¡£"""
        print("=== å¼€å§‹åˆ†æISOç¨‹åºæ–‡æ¡£ ===")
        print(f"æ‰«æè·¯å¾„: {self.base_path}")
        
        # è·å–æ‰€æœ‰ä¸»è¦ç¨‹åºæ–‡æ¡£
        main_docs = []
        for root, dirs, files in os.walk(self.base_path):
            for file in files:
                if file.startswith('HQ-QP-') and file.endswith('.doc'):
                    full_path = os.path.join(root, file)
                    main_docs.append((full_path, file))
        
        print(f"æ‰¾åˆ°çš„æ–‡æ¡£è·¯å¾„ç¤ºä¾‹:")
        for i, (path, name) in enumerate(main_docs[:3]):
            print(f"  {i+1}. {name}")
            print(f"     è·¯å¾„: {path}")
        
        print(f"\næ‰¾åˆ° {len(main_docs)} ä¸ªä¸»è¦ç¨‹åºæ–‡æ¡£")
        
        suitable_count = 0
        unsuitable_count = 0
        
        for file_path, filename in sorted(main_docs):
            print(f"\nåˆ†ææ–‡æ¡£: {filename}")
            
            # åŸºäºæ–‡ä»¶ååˆ†æé€‚ç”¨æ€§
            analysis = self.analyze_document_by_filename(filename, file_path)
            
            # ä¿å­˜åˆ†æç»“æœ
            result = {
                'filename': filename,
                'file_path': file_path,
                'relative_path': os.path.relpath(file_path, self.base_path),
                'analysis': analysis
            }
            
            self.analysis_results.append(result)
            
            # ç»Ÿè®¡ç»“æœ
            if analysis['suitable']:
                suitable_count += 1
                print(f"  âœ… é€‚åˆç”Ÿæˆæµç¨‹å›¾ - {analysis['reason']}")
            else:
                unsuitable_count += 1
                print(f"  âŒ ä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾ - {analysis['reason']}")
            
            print(f"  ğŸ“‚ åˆ†ç±»: {analysis['category']}")
            print(f"  ğŸ¯ åŒ¹é…æ¨¡å¼: {analysis.get('pattern_matched', 'æ— ')}")
            print(f"  ğŸ“Š ç½®ä¿¡åº¦: {analysis['confidence']}%")
            print(f"  ğŸ” é€‚ç”¨æ€§çº§åˆ«: {analysis['suitability_level']}")
        
        print(f"\n=== åˆ†æå®Œæˆ ===")
        print(f"é€‚åˆç”Ÿæˆæµç¨‹å›¾: {suitable_count} ä¸ª")
        print(f"ä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾: {unsuitable_count} ä¸ª")
        print(f"æ€»è®¡: {len(main_docs)} ä¸ª")
        print(f"é€‚ç”¨ç‡: {suitable_count/len(main_docs)*100:.1f}%")
        
        return self.analysis_results
    
    def generate_analysis_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.analysis_results:
            print("æ²¡æœ‰åˆ†æç»“æœï¼Œè¯·å…ˆè¿è¡Œanalyze_all_documents()")
            return
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_content = []
        report_content.append("# ISOæ–‡æ¡£æµç¨‹å›¾é€‚ç”¨æ€§åˆ†ææŠ¥å‘Š")
        report_content.append(f"\n**åˆ†ææ—¶é—´**: {self.get_current_time()}")
        report_content.append(f"**åˆ†ææ–‡æ¡£æ•°é‡**: {len(self.analysis_results)}")
        report_content.append(f"**åˆ†ææ–¹æ³•**: åŸºäºæ–‡ä»¶åå’ŒISOæ ‡å‡†ç¨‹åºç‰¹å¾çš„æ™ºèƒ½åˆ¤æ–­")
        
        # ç»Ÿè®¡ä¿¡æ¯
        suitable_docs = [r for r in self.analysis_results if r['analysis']['suitable']]
        unsuitable_docs = [r for r in self.analysis_results if not r['analysis']['suitable']]
        
        report_content.append(f"\n## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ")
        report_content.append(f"- âœ… é€‚åˆç”Ÿæˆæµç¨‹å›¾: **{len(suitable_docs)}** ä¸ª")
        report_content.append(f"- âŒ ä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾: **{len(unsuitable_docs)}** ä¸ª")
        report_content.append(f"- ğŸ“ˆ é€‚ç”¨ç‡: **{len(suitable_docs)/len(self.analysis_results)*100:.1f}%**")
        
        # é€‚ç”¨æ€§çº§åˆ«ç»Ÿè®¡
        level_stats = defaultdict(int)
        for result in self.analysis_results:
            level_stats[result['analysis']['suitability_level']] += 1
        
        report_content.append(f"\n## ğŸ¯ é€‚ç”¨æ€§çº§åˆ«ç»Ÿè®¡")
        for level, count in sorted(level_stats.items()):
            report_content.append(f"- {level}: {count} ä¸ª")
        
        # åˆ†ç±»ç»Ÿè®¡
        category_stats = defaultdict(int)
        for result in self.analysis_results:
            category_stats[result['analysis']['category']] += 1
        
        report_content.append(f"\n## ğŸ“‚ æ–‡æ¡£åˆ†ç±»ç»Ÿè®¡")
        for category, count in sorted(category_stats.items()):
            report_content.append(f"- {category}: {count} ä¸ª")
        
        # é€‚åˆç”Ÿæˆæµç¨‹å›¾çš„æ–‡æ¡£åˆ—è¡¨
        report_content.append(f"\n## âœ… é€‚åˆç”Ÿæˆæµç¨‹å›¾çš„æ–‡æ¡£ ({len(suitable_docs)} ä¸ª)")
        
        # æŒ‰é€‚ç”¨æ€§çº§åˆ«åˆ†ç»„
        high_docs = [r for r in suitable_docs if r['analysis']['suitability_level'] == 'high']
        medium_docs = [r for r in suitable_docs if r['analysis']['suitability_level'] == 'medium']
        default_docs = [r for r in suitable_docs if r['analysis']['suitability_level'] == 'default']
        
        if high_docs:
            report_content.append(f"\n### ğŸ”¥ é«˜é€‚ç”¨æ€§æ–‡æ¡£ ({len(high_docs)} ä¸ª)")
            for result in high_docs:
                analysis = result['analysis']
                report_content.append(f"\n#### {result['filename']}")
                report_content.append(f"- **åˆ†ç±»**: {analysis['category']}")
                report_content.append(f"- **ç½®ä¿¡åº¦**: {analysis['confidence']}%")
                report_content.append(f"- **åŒ¹é…æ¨¡å¼**: {analysis['pattern_matched']}")
                report_content.append(f"- **ç†ç”±**: {analysis['reason']}")
        
        if medium_docs:
            report_content.append(f"\n### ğŸ¯ ä¸­ç­‰é€‚ç”¨æ€§æ–‡æ¡£ ({len(medium_docs)} ä¸ª)")
            for result in medium_docs:
                analysis = result['analysis']
                report_content.append(f"\n#### {result['filename']}")
                report_content.append(f"- **åˆ†ç±»**: {analysis['category']}")
                report_content.append(f"- **ç½®ä¿¡åº¦**: {analysis['confidence']}%")
                report_content.append(f"- **åŒ¹é…æ¨¡å¼**: {analysis['pattern_matched']}")
                report_content.append(f"- **ç†ç”±**: {analysis['reason']}")
        
        if default_docs:
            report_content.append(f"\n### ğŸ“‹ é»˜è®¤é€‚ç”¨æ€§æ–‡æ¡£ ({len(default_docs)} ä¸ª)")
            for result in default_docs:
                analysis = result['analysis']
                report_content.append(f"\n#### {result['filename']}")
                report_content.append(f"- **åˆ†ç±»**: {analysis['category']}")
                report_content.append(f"- **ç½®ä¿¡åº¦**: {analysis['confidence']}%")
                report_content.append(f"- **ç†ç”±**: {analysis['reason']}")
        
        # ä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾çš„æ–‡æ¡£åˆ—è¡¨
        if unsuitable_docs:
            report_content.append(f"\n## âŒ ä¸é€‚åˆç”Ÿæˆæµç¨‹å›¾çš„æ–‡æ¡£ ({len(unsuitable_docs)} ä¸ª)")
            for result in unsuitable_docs:
                analysis = result['analysis']
                report_content.append(f"\n### {result['filename']}")
                report_content.append(f"- **åˆ†ç±»**: {analysis['category']}")
                report_content.append(f"- **ç½®ä¿¡åº¦**: {analysis['confidence']}%")
                report_content.append(f"- **åŒ¹é…æ¨¡å¼**: {analysis.get('pattern_matched', 'æ— ')}")
                report_content.append(f"- **ç†ç”±**: {analysis['reason']}")
        
        # ç”Ÿæˆæµç¨‹å›¾å»ºè®®æ¸…å•
        report_content.append(f"\n## ğŸ¨ æµç¨‹å›¾ç”Ÿæˆå»ºè®®æ¸…å•")
        report_content.append(f"\nåŸºäºåˆ†æç»“æœï¼Œå»ºè®®æŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§ç”Ÿæˆæµç¨‹å›¾ï¼š")
        
        if high_docs:
            report_content.append(f"\n### ä¼˜å…ˆçº§1ï¼šé«˜é€‚ç”¨æ€§æ–‡æ¡£ ({len(high_docs)} ä¸ª)")
            for i, result in enumerate(high_docs, 1):
                report_content.append(f"{i}. {result['filename']} - {result['analysis']['category']}")
        
        if medium_docs:
            report_content.append(f"\n### ä¼˜å…ˆçº§2ï¼šä¸­ç­‰é€‚ç”¨æ€§æ–‡æ¡£ ({len(medium_docs)} ä¸ª)")
            for i, result in enumerate(medium_docs, 1):
                report_content.append(f"{i}. {result['filename']} - {result['analysis']['category']}")
        
        if default_docs:
            report_content.append(f"\n### ä¼˜å…ˆçº§3ï¼šé»˜è®¤é€‚ç”¨æ€§æ–‡æ¡£ ({len(default_docs)} ä¸ª)")
            for i, result in enumerate(default_docs, 1):
                report_content.append(f"{i}. {result['filename']} - {result['analysis']['category']}")
        
        # ä¿å­˜æŠ¥å‘Š
        self.output_path.mkdir(parents=True, exist_ok=True)
        report_file = self.output_path / "ISOæ–‡æ¡£æµç¨‹å›¾é€‚ç”¨æ€§åˆ†ææŠ¥å‘Š.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        print(f"\nğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
        json_file = self.output_path / "ISOæ–‡æ¡£åˆ†æç»“æœ.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ è¯¦ç»†æ•°æ®å·²ä¿å­˜: {json_file}")
        
        return suitable_docs
    
    def get_current_time(self):
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = ISODocumentAnalyzer()
    
    # åˆ†ææ‰€æœ‰æ–‡æ¡£
    results = analyzer.analyze_all_documents()
    
    # ç”ŸæˆæŠ¥å‘Š
    suitable_docs = analyzer.generate_analysis_report()
    
    print(f"\n=== ç¬¬ä¸€é˜¶æ®µï¼šæ–‡æ¡£æ‰«æä¸åˆ†æ å®Œæˆ ===")
    print(f"å…±åˆ†æ {len(results)} ä¸ªISOç¨‹åºæ–‡æ¡£")
    print(f"å…¶ä¸­ {len(suitable_docs)} ä¸ªé€‚åˆç”Ÿæˆæµç¨‹å›¾")
    print(f"é€‚ç”¨ç‡: {len(suitable_docs)/len(results)*100:.1f}%")
    
    return suitable_docs

if __name__ == "__main__":
    main()